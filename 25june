import os
import logging
import pyodbc
from bs4 import BeautifulSoup

# Configure logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# Database configuration
database = 'abianalytics'
server = 'your_server_name'  # Replace with your actual server name

# Function to connect to the database
def connect_to_database():
    try:
        conn = pyodbc.connect(
            f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;'
        )
        logging.info("Connected to the database successfully.")
        return conn
    except pyodbc.Error as ex:
        sqlstate = ex.args[0]
        if sqlstate == '08001':
            logging.error("Network-related or instance-specific error occurred while establishing a connection to SQL Server.")
        logging.error(ex)
        raise

# Function to parse and extract information from HTML content
def extract_information_from_html(content):
    soup = BeautifulSoup(content, 'html.parser')
    logging.debug("Raw HTML Content Loaded")
    print("Raw HTML Content Loaded")
    
    table = soup.find('table')
    if not table:
        logging.error("No table found in the HTML content")
        print("No table found in the HTML content")
        return {'main': {}, 'pairs': []}
    
    extracted_data = {
        'main': {},
        'pairs': []
    }
    
    rows = table.find_all('tr')
    logging.debug(f"Found {len(rows)} rows in the table")
    print(f"Found {len(rows)} rows in the table")

    i = 0
    while i < len(rows):
        cells = rows[i].find_all('td')
        logging.debug(f"Row {i} cells: {cells}")
        print(f"Row {i} cells: {cells}")

        if len(cells) == 2:
            # Extract key-value pair
            key = cells[0].get_text(strip=True).replace(" ", "").replace("&", "And")
            value = cells[1].get_text(strip=True)
            logging.debug(f"Key: {key}, Value: {value}")
            print(f"Key: {key}, Value: {value}")
            extracted_data['main'][key] = value
            i += 1
        elif len(cells) == 3:
            # Process pairs from 3-cell rows
            while i < len(rows) and len(rows[i].find_all('td')) == 3:
                if i + 1 >= len(rows):
                    logging.error("Unmatched row at the end of the table")
                    break
                if i + 2 >= len(rows):
                    logging.error("Missing third row for pairing")
                    break

                row1_cells = rows[i].find_all('td')
                row2_cells = rows[i + 1].find_all('td')
                row3_cells = rows[i + 2].find_all('td')

                for idx in range(3):
                    value1 = row2_cells[idx].get_text(strip=True)
                    value2 = row3_cells[idx].get_text(strip=True)
                    pair = f"{row1_cells[idx].get_text(strip=True)}-{value1}-{value2}"
                    logging.debug(f"Extracted pair: {pair}")
                    print(f"Extracted pair: {pair}")
                    extracted_data['pairs'].append(pair)
                
                i += 3
        else:
            logging.error(f"Unexpected number of cells in row {i}")
            print(f"Unexpected number of cells in row {i}")
            i += 1

    logging.debug(f"Extracted Data: {extracted_data}")
    print(f"Extracted Data: {extracted_data}")
    return extracted_data

# Function to read HTML file and process data
def process_html_file(file_path):
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
        content = file.read()
        data = extract_information_from_html(content)
        logging.info(f"Extracted Data: {data}")
        print(f"Extracted Data: {data}")

# Main execution
if __name__ == "__main__":
    file_path = 'htmltable.html'
    process_html_file(file_path)

---------------------
def insert_data_into_database(data):
    conn = connect_to_database()
    cursor = conn.cursor()

    insert_main_query = """
    INSERT INTO dbo.firlookup (PriorityIncidentNumber, IncidentDescription, LineOfBusiness, BusinessImpact, OnlineBatch)
    VALUES (?, ?, ?, ?, ?)
    """

    cursor.execute(insert_main_query, 
                   data['main'].get('PriorityIncidentNumber'), 
                   data['main'].get('IncidentDescription'), 
                   data['main'].get('LineOfBusiness'), 
                   data['main'].get('BusinessImpact'), 
                   data['main'].get('OnlineBatch'))
    
    conn.commit()
    cursor.close()
    conn.close()
    logging.info("Data inserted into the database")
..............

import os
import logging
from bs4 import BeautifulSoup

# Configure logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# Function to parse and extract information from HTML content
def extract_information_from_html(content):
    soup = BeautifulSoup(content, 'html.parser')
    logging.debug("Raw HTML Content Loaded")
    
    table = soup.find('table')
    if not table:
        logging.error("No table found in the HTML content")
        return {'main': {}, 'status_updates': []}
    
    extracted_data = {
        'main': {},
        'status_updates': []
    }
    
    rows = table.find_all('tr')
    logging.debug(f"Found {len(rows)} rows in the table")

    keys = ["PriorityIncidentNumber", "IncidentDescription", "LineOfBusiness", "BusinessImpact", "OnlineBatch"]
    headers = ["CurrentStatus", "DateTime", "StatusUpdates"]
    current_status_update = {}

    skip_next_row = False

    for i, row in enumerate(rows):
        cells = row.find_all('td')
        logging.debug(f"Found {len(cells)} cells in the row")

        if skip_next_row:
            skip_next_row = False
            if len(cells) == 3:
                current_status_update["CurrentStatus"] = cells[0].get_text(strip=True)
                current_status_update["DateTime"] = cells[1].get_text(strip=True)
                current_status_update["StatusUpdates"] = cells[2].get_text(strip=True)
                extracted_data['status_updates'].append(current_status_update)
                current_status_update = {}
            continue

        if len(cells) == 2:
            key = cells[0].get_text(strip=True).replace(" ", "").replace("&", "And")
            value = cells[1].get_text(strip=True)
            logging.debug(f"Key: {key}, Value: {value}")
            if key in keys:
                extracted_data['main'][key] = value
            elif key in headers:
                skip_next_row = True

    logging.debug(f"Extracted Data: {extracted_data}")
    return extracted_data

# Function to read HTML file and extract data
def process_html_file(file_path):
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
        content = file.read()
        data = extract_information_from_html(content)
        logging.info(f"Main Data: {data['main']}")
        logging.info(f"Status Updates: {data['status_updates']}")

# Main execution
if __name__ == "__main__":
    file_path = 'htmltable.html'
    process_html_file(file_path)
