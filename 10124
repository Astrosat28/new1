import re
import heapq
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize

# Ensure you have the necessary components installed
nltk.download('punkt')
nltk.download('stopwords')

def summarize_text(text):
    # Tokenize the text into sentences
    sentences = sent_tokenize(text)

    # Tokenize words and create a frequency table
    frequency_table = dict()
    stopWords = set(stopwords.words("english"))
    words = word_tokenize(text)

    for word in words:
        word = word.lower()
        if word not in stopWords:
            if word in frequency_table:
                frequency_table[word] += 1
            else:
                frequency_table[word] = 1

    # Score sentences based on frequency
    sentence_scores = dict()

    for sentence in sentences:
        for word, freq in frequency_table.items():
            if word in sentence.lower():
                if sentence in sentence_scores:
                    sentence_scores[sentence] += freq
                else:
                    sentence_scores[sentence] = freq

    # Get the average score for a sentence
    average_score = sum(sentence_scores.values()) / len(sentence_scores)

    # Extract summary: sentences with a score above the average
    summary = heapq.nlargest(3, sentence_scores, key=sentence_scores.get)

    return ' '.join(summary)

# Example usage
text_description = most_similar_incident['description']
text_close_notes = most_similar_incident['close_notes']
text_comments = most_similar_incident['comments']

description_summary = summarize_text(text_description)
close_notes_summary = summarize_text(text_close_notes)
comments_summary = summarize_text(text_comments)

print("Summary of Description:")
print(description_summary)
print("\nSummary of Close Notes:")
print(close_notes_summary)
print("\nSummary of Comments:")
print(comments_summary)


# Pseudocode example
from langchain.llms import OpenAI
from some_search_library import search_similar_incidents
from preprocessing import preprocess_data

# Preprocess new incident data
new_incident_data = preprocess_data(new_incident_data)

# Find similar incidents from the database
similar_incidents = search_similar_incidents(new_incident_data, historical_data)

# Format the data for the AI
ai_input = format_for_ai(new_incident_data, similar_incidents)

# Initialize Langchain with your chosen model
llm = OpenAI(api_key="your-api-key")

# Generate a solution
ai_solution = llm.complete(prompt=ai_input)

# Output the solution
print(ai_solution)







import pandas as pd
from transformers import BertTokenizer, BertModel, BertForSequenceClassification, pipeline
from sklearn.metrics.pairwise import cosine_similarity
import torch

# Load your dataset
df = pd.read_csv(r'C:/aiml python/your_dataset.csv')
pd.set_option('display.max_colwidth', None)
pd.set_option('display.max_columns', None)

# Initialize BERT
model_name = 'bert-base-nli-mean-tokens'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# Function to create embeddings
def create_embeddings(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1)

# Embed all incidents
incident_embeddings = df.apply(lambda row: create_embeddings(row['incident_number'] + ' ' + row['problem'] + ' ' + row['work_notes'] + ' ' + row['U_cause_description']), axis=1)

# New incident
new_incident = 'Your new incident text here'
new_incident_embedding = create_embeddings(new_incident)

# Find most similar incident
similarities = cosine_similarity(new_incident_embedding, incident_embeddings)
most_similar_idx = similarities.argmax()

# Get the most similar incident
most_similar_incident = df.iloc[most_similar_idx]

# Summarization with BERT
summarizer = pipeline("summarization", model="bert-large-cnn")
fields_to_summarize = ['problem', 'work_notes', 'U_cause_description']

for field in fields_to_summarize:
    summarized_text = summarizer(most_similar_incident[field], max_length=130, min_length=30, do_sample=False)
    print(f"\nSummarized {field}: {summarized_text[0]['summary_text']}")



newness new
import json
from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

# Initialize the model and tokenizer
tokenizer = T5Tokenizer.from_pretrained('t5-small')
model = T5ForConditionalGeneration.from_pretrained('t5-small')

def summarize_text(text, max_length=200):
    # Tokenize the text
    inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=512, truncation=True)
    # Generate summary
    summary_ids = model.generate(inputs, max_length=max_length, length_penalty=2.0, num_beams=4, early_stopping=True)
    # Decode the summary
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

def lambda_handler(event, context):
    try:
        # Get the text to summarize from the event body
        text_to_summarize = event.get('text', '')

        # Perform summarization
        summary = summarize_text(text_to_summarize)

        # Return the summarized text
        return {
            'statusCode': 200,
            'body': json.dumps({'summary': summary})
        }
    except Exception as e:
        # Handle any exceptions
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }